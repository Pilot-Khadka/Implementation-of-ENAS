{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:28.681322411Z",
     "start_time": "2023-12-08T16:02:28.519781820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9310,  1.9308, -0.8158,  0.1399, -1.9489,  0.9859, -1.4549,  0.3011,\n",
      "          0.2422, -1.1729]])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(1,10)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(10, 100)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 10\n",
    "hidden_size =100\n",
    "\n",
    "x = nn.LSTM(input_size=input_size,\n",
    "            hidden_size=100)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:28.696167439Z",
     "start_time": "2023-12-08T16:02:28.676122102Z"
    }
   },
   "id": "e9ee39223a238b4d"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(10, 100)\n"
     ]
    }
   ],
   "source": [
    "embed = nn.Embedding(num_embeddings=10,\n",
    "                     embedding_dim=100)\n",
    "print(embed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:28.831430464Z",
     "start_time": "2023-12-08T16:02:28.702158433Z"
    }
   },
   "id": "2c3e1f57782466f0"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9, 1, 6, 8, 6, 2, 7, 2, 5, 7]])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "random_data = torch.randint(low=1,\n",
    "                            high=10,\n",
    "                            size=(1,10))\n",
    "print(random_data)\n",
    "print(random_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:28.850177296Z",
     "start_time": "2023-12-08T16:02:28.776223913Z"
    }
   },
   "id": "444bdb13cca596e2"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "embed_out = embed(random_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:28.949810536Z",
     "start_time": "2023-12-08T16:02:28.857475176Z"
    }
   },
   "id": "a4a0489d45435998"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "print(embed_out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:28.974700827Z",
     "start_time": "2023-12-08T16:02:28.908429596Z"
    }
   },
   "id": "b8ff9a5e4f03590f"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 100])\n"
     ]
    }
   ],
   "source": [
    "# if input size not 1,10\n",
    "random_data2 = torch.randint(1,10,size=(4,5))\n",
    "embed_out2 = embed(random_data2)\n",
    "print(embed_out2.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:29.038234104Z",
     "start_time": "2023-12-08T16:02:28.958217441Z"
    }
   },
   "id": "15734ccdc2a8a12e"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3339, 0.4873, 0.2267],\n",
      "        [0.9517, 1.1873, 0.4183],\n",
      "        [0.1010, 0.2022, 0.1148]])\n",
      "tensor([[0.3649],\n",
      "        [0.8732],\n",
      "        [0.1586]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Matrix multiplication\n",
    "mat1 = torch.rand(3,3)\n",
    "mat2 = torch.rand(3,3)\n",
    "\n",
    "res1 = torch.matmul(mat1,mat2) \n",
    "print(res1)\n",
    "\n",
    "linear = nn.Linear(3,1)\n",
    "res2 = linear(mat1)\n",
    "print(res2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:29.144333823Z",
     "start_time": "2023-12-08T16:02:29.041563481Z"
    }
   },
   "id": "a5d60a59b18d493"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 1x10)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 13\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# dim0: first dim to be transposed\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# dim1: second dim to be transposed\u001B[39;00m\n\u001B[1;32m     11\u001B[0m decoder_transpose \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtranspose(decoder_output,\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m dot_product \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencoder_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m attention_weights \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSoftmax(dot_product)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEncoder output:\u001B[39m\u001B[38;5;124m'\u001B[39m,encoder_output)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 1x10)"
     ]
    }
   ],
   "source": [
    "# Attention implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "encoder_output = torch.rand(1,10)\n",
    "decoder_output = torch.rand(1,10)\n",
    "\n",
    "# dim0: first dim to be transposed\n",
    "# dim1: second dim to be transposed\n",
    "decoder_transpose = torch.transpose(decoder_output,0,1)\n",
    "\n",
    "dot_product = torch.matmul(encoder_output, decoder_output)\n",
    "\n",
    "\n",
    "attention_weights = nn.Softmax(dot_product)\n",
    "print('Encoder output:',encoder_output)\n",
    "print('Decoder output',decoder_output)\n",
    "print('Dot product:', dot_product)\n",
    "print('Attention weights:',attention_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:29.162799400Z",
     "start_time": "2023-12-08T16:02:29.120427888Z"
    }
   },
   "id": "b683301c2f2dcca0"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder transform shape torch.Size([1, 10])\n",
      "Dot product shape: torch.Size([1, 1])\n",
      "Attention weights shape: torch.Size([1, 1])\n",
      "Attended encoder output: torch.Size([1, 10])\n",
      "Attended encoder output: tensor([[ 0.1028,  0.2465, -0.1070, -0.1965, -0.2749,  0.4547,  0.3668,  0.0807,\n",
      "          0.2115,  0.2449]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self,encoder_output, decoder_output):\n",
    "        \n",
    "        encoder_transform = self.linear(encoder_output)\n",
    "        print(\"Encoder transform shape\",encoder_transform.shape)\n",
    "        \n",
    "        dot_product = torch.matmul(encoder_transform , decoder_output.T)\n",
    "        print(\"Dot product shape:\", dot_product.shape)\n",
    "        \n",
    "        # dim=0 for tensor, else->softmax type\n",
    "        attention_weights = self.softmax(dot_product)\n",
    "        print('Attention weights shape:', attention_weights.shape)\n",
    "        \n",
    "        # attention_weights = torch.tensor(attention_weights)\n",
    "        \n",
    "        # weighted sum of encoder output\n",
    "        attended_encoder_output = torch.matmul(attention_weights, encoder_transform)\n",
    "        # attention_weights * encoder_transform\n",
    "        \n",
    "        print('Attended encoder output:', attended_encoder_output.shape)\n",
    "        \n",
    "        return attention_weights, attended_encoder_output\n",
    "    \n",
    "hidden_size = 10\n",
    "\n",
    "attention_model = Attention(hidden_size)\n",
    "\n",
    "attention_weights, attended_encoder_output = attention_model(encoder_output, decoder_output)\n",
    "\n",
    "# print(\"Encoder output shape:\", encoder_output)\n",
    "# print(\"Decoder output shape:\", decoder_output)\n",
    "# print(\"Attention weights:\", attention_weights)\n",
    "print(\"Attended encoder output:\", attended_encoder_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:29.360370391Z",
     "start_time": "2023-12-08T16:02:29.156381811Z"
    }
   },
   "id": "1024695fada23ea0"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embded size: (4, 3)\n",
      "k size:(4, 3), q size:(4, 3), v size(4, 3)\n",
      "Scores shape: (4, 4)\n",
      "[[0.98522025 1.74174051 0.75652026]\n",
      " [0.90965265 1.40965265 0.5       ]\n",
      " [0.99851226 1.75849334 0.75998108]\n",
      " [0.99560386 1.90407309 0.90846923]]\n",
      "Attention shape: (4, 3)\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import random\n",
    "from numpy import dot\n",
    "from scipy.special import softmax\n",
    "\n",
    "word_1 = array([1, 0, 0])\n",
    "word_2 = array([0, 1, 0])\n",
    "word_3 = array([1, 1, 0])\n",
    "word_4 = array([0, 0, 1])\n",
    "\n",
    "words = array([word_1, word_2, word_3, word_4])\n",
    "print(\"Word embded size:\", words.shape)\n",
    "\n",
    "random.seed(42)\n",
    "W_Q = random.randint(3, size=(3, 3))\n",
    "W_K = random.randint(3, size=(3, 3))\n",
    "W_V = random.randint(3, size=(3, 3))\n",
    "\n",
    "Q = words @ W_Q\n",
    "K = words @ W_K\n",
    "V = words @ W_V\n",
    "print('k size:{}, q size:{}, v size{}'.format(K.shape,Q.shape,V.shape))\n",
    "\n",
    "scores = Q @ K.transpose()\n",
    "print('Scores shape:', scores.shape)\n",
    "\n",
    "weights = softmax(scores / K.shape[1] ** 0.5, axis=1)\n",
    "attention = weights @ V\n",
    "print(attention)\n",
    "print('Attention shape:', attention.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:29.615609033Z",
     "start_time": "2023-12-08T16:02:29.318026257Z"
    }
   },
   "id": "7a8a048d387cfa96"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:29.624233147Z",
     "start_time": "2023-12-08T16:02:29.608313108Z"
    }
   },
   "id": "a13e724724f25e77"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words shape: torch.Size([4, 3])\n",
      "Embedding(4, 3)\n",
      "LSTM(3, 35)\n",
      "Linear(in_features=35, out_features=3, bias=True)\n",
      "torch.Size([4, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from numpy import array\n",
    "\n",
    "\n",
    "word_1 = array([1, 0, 0])\n",
    "word_2 = array([0, 1, 0])\n",
    "word_3 = array([1, 1, 0])\n",
    "word_4 = array([0, 0, 1])\n",
    "\n",
    "words = array([word_1, word_2, word_3, word_4])\n",
    "words = torch.tensor(words)\n",
    "print(\"Words shape:\",words.shape)\n",
    "\n",
    "# initialize a single layer LSTM with 35 hidden layers\n",
    "# 4 words in vocab\n",
    "# 3 dimensional embedding\n",
    "\n",
    "embed = nn.Embedding(num_embeddings=4,\n",
    "                     embedding_dim=3)\n",
    "print(embed)\n",
    "lstm = nn.LSTM(input_size=3,hidden_size=35)\n",
    "print(lstm)\n",
    "soft = nn.Linear(in_features=35,out_features=3)\n",
    "print(soft)\n",
    "# initialize hidden and cell state of lstm\n",
    "\n",
    "# nature of h_0 and c_0:\n",
    "#     -> shape: ( D*num_layers, Hout)\n",
    "#         D = 1 (2 for bidirectional)\n",
    "#         num_layers = 1 for single layer\n",
    "#         Hout = hidden_size\n",
    "\n",
    "h_0 = torch.zeros(1,3,35)\n",
    "c_0 = torch.zeros(1,3,35)\n",
    "input_data = embed(words)\n",
    "print(input_data.shape)\n",
    "# print(input_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:29.633837845Z",
     "start_time": "2023-12-08T16:02:29.608656732Z"
    }
   },
   "id": "82621b81271f1aea"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 shape torch.Size([4, 3, 35])\n",
      "torch.Size([3, 35])\n",
      "c1 shape 2\n"
     ]
    }
   ],
   "source": [
    "h_1, c_1 = lstm(input_data,(h_0, c_0))\n",
    "print(\"h1 shape\",h_1.shape)\n",
    "print(h_1[-1].shape)\n",
    "# tuple containing ltm\n",
    "# print(c_1)\n",
    "print('c1 shape',len(c_1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:29.635142428Z",
     "start_time": "2023-12-08T16:02:29.608892798Z"
    }
   },
   "id": "e7495eca01ad091"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# attention mechanism\n",
    "# query -->> is the output of previous lstm state\n",
    "\n",
    "# output of previous LSTM , since first Layer, init to zero\n",
    "prev_h = torch.zeros(h_1.shape)\n",
    "\n",
    "w_query = nn.Linear(35,35)\n",
    "\n",
    "query = w_query(prev_h)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:29.793725691Z",
     "start_time": "2023-12-08T16:02:29.625376464Z"
    }
   },
   "id": "a28fe39ae9da46bf"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 35])\n"
     ]
    }
   ],
   "source": [
    "# size of query should be 4x3x35 \n",
    "print(query.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:29.806342613Z",
     "start_time": "2023-12-08T16:02:29.684565697Z"
    }
   },
   "id": "d45718a03515d06c"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "LSTM(3, 35)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:29.924192234Z",
     "start_time": "2023-12-08T16:02:29.805618908Z"
    }
   },
   "id": "c07e560ab12f5d88"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "prev_h = torch.zeros(1,3,35)\n",
    "prev_c = torch.zeros(1,3,35)\n",
    "\n",
    "\n",
    "# attention mechanism\n",
    "attention_weights1 = nn.Linear(35,35)\n",
    "\n",
    "all_h = []\n",
    "all_weighted_h = []\n",
    "for i in range(5):\n",
    "    next_h, next_c = lstm(input_data,(prev_h,prev_c))\n",
    "    # saving hidden states\n",
    "    all_h.append(next_h[-1])\n",
    "    # compute weighted version of hidden states\n",
    "    all_weighted_h.append(torch.matmul(next_h[-1],attention_weights1.weight))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:30.005935815Z",
     "start_time": "2023-12-08T16:02:29.883983944Z"
    }
   },
   "id": "b95ac16545b9aecb"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9, 6, 8, 2, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sequence = torch.randint(1,10,size=(1,6))\n",
    "print(sequence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:30.042961117Z",
     "start_time": "2023-12-08T16:02:29.953620521Z"
    }
   },
   "id": "42849224d102eafc"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m embed \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mEmbedding(\u001B[38;5;241m6\u001B[39m,\u001B[38;5;241m16\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m embedded_sequence \u001B[38;5;241m=\u001B[39m \u001B[43membed\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequence\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# seperates tensor from computational graph \u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# i.e does not require gradient\u001B[39;00m\n\u001B[1;32m      6\u001B[0m embed_detach \u001B[38;5;241m=\u001B[39m embedded_sequence\u001B[38;5;241m.\u001B[39mdetach()\n",
      "File \u001B[0;32m~/miniconda3/envs/jupyter/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/jupyter/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/jupyter/lib/python3.8/site-packages/torch/nn/modules/sparse.py:162\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/jupyter/lib/python3.8/site-packages/torch/nn/functional.py:2233\u001B[0m, in \u001B[0;36membedding\u001B[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[1;32m   2227\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[1;32m   2228\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[1;32m   2229\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[1;32m   2230\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[1;32m   2231\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[1;32m   2232\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[0;32m-> 2233\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mIndexError\u001B[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "embed = torch.nn.Embedding(6,16)\n",
    "embedded_sequence = embed(sequence)\n",
    "\n",
    "# seperates tensor from computational graph \n",
    "# i.e does not require gradient\n",
    "embed_detach = embedded_sequence.detach()\n",
    "print(embedded_sequence.shape)\n",
    "print(embed_detach.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T16:02:30.421047985Z",
     "start_time": "2023-12-08T16:02:30.018922831Z"
    }
   },
   "id": "76923015403e285f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Wq = Wk = dk x d or dq x d\n",
    "# since we calculate dot product between the query and key vector\n",
    "# \n",
    "# Wv = dv x d\n",
    "# where d = size of each word vector\n",
    "#         = 6 in above sequence\n",
    "# dq, dk, dv can be arbitary\n",
    "# d = embed_sequence[1]\n",
    "\n",
    "w_query = torch.nn.Linear(in_features=24,out_features=16)\n",
    "w_key = torch.nn.Linear(in_features=24,out_features=16)\n",
    "w_value = torch.nn.Linear(in_features=24,out_features=16)\n",
    "\n",
    "# computation of unnormalized attention weights\n",
    "\n",
    "# for a single input sentence\n",
    "query2 = torch.matmul(embed_detach[-1],w_query.weight)\n",
    "print(query2.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-08T16:02:30.388987835Z"
    }
   },
   "id": "546ab636cb81431a"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4a67bb5cd2418e07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w_query2 = torch.nn.Parameter(torch.rand(24,6))\n",
    "print(w_query2.shape)\n",
    "\n",
    "# pytorch's linear layer stores weights in the format (out_feature, in_feature) so, need to Transpose during matrix multiplication\n",
    "print(w_query.weight.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-08T16:02:30.389244714Z"
    }
   },
   "id": "c101adbd93b2634c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keys = torch.matmul(embed_detach[-1],w_query.weight)\n",
    "values = torch.matmul(embed_detach[-1], w_value.weight)\n",
    "print(keys.shape)\n",
    "print(values.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-08T16:02:30.389374272Z"
    }
   },
   "id": "1f73bb240f4a8240"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute matmul between queries(output of last hidden lstm, RNN, encoder)\n",
    "\n",
    "omega = torch.matmul(query2, keys.T)\n",
    "print(omega.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-08T16:02:30.389554185Z"
    }
   },
   "id": "aac80842e7c02988"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Computation of attention scores\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights_all = F.softmax(omega / 24**0.5, dim=0)\n",
    "print(attention_weights_all)\n",
    "print(attention_weights_all.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-08T16:02:30.389858557Z"
    }
   },
   "id": "5cfcbcc6151bf673"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# context vector is the attention weighted version of the original query input \n",
    "\n",
    "context_vector = torch.matmul(attention_weights_all,values)\n",
    "print(context_vector)\n",
    "print(context_vector.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-08T16:02:30.390090922Z"
    }
   },
   "id": "d531f73a4892d94a"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 100])\n",
      "(tensor([[-0.0391,  0.0237, -0.0291, -0.2388,  0.0705,  0.1018,  0.0432,  0.1690,\n",
      "         -0.0967,  0.1817,  0.0355, -0.1110,  0.2740, -0.0509, -0.0468,  0.1163,\n",
      "          0.0146, -0.0719,  0.0260, -0.0145,  0.0454,  0.0019,  0.1160, -0.1382,\n",
      "         -0.0287, -0.0772, -0.1221,  0.1555,  0.0802, -0.1099,  0.1374,  0.1561,\n",
      "         -0.0966,  0.0096,  0.0791,  0.0589, -0.0927,  0.0040, -0.0584, -0.0419,\n",
      "         -0.0044,  0.0862, -0.0450,  0.0529, -0.0382, -0.0347,  0.0328, -0.0913,\n",
      "         -0.1001, -0.1548, -0.0102,  0.0643, -0.0453, -0.0988,  0.2477,  0.1163,\n",
      "         -0.0070,  0.0392,  0.0223, -0.0076, -0.1271,  0.1051, -0.1325,  0.0017,\n",
      "          0.0294,  0.1162,  0.1340,  0.0202, -0.0006,  0.0756,  0.0003,  0.1321,\n",
      "          0.1171, -0.0178, -0.0402, -0.0771, -0.0311, -0.1046,  0.0575,  0.0502,\n",
      "          0.0278,  0.0284, -0.0339, -0.0546, -0.0714, -0.0794,  0.1206,  0.0172,\n",
      "          0.0972,  0.0396, -0.1417,  0.1030,  0.0654, -0.0376, -0.0060,  0.1955,\n",
      "          0.0283, -0.0784, -0.0606,  0.0775]], grad_fn=<SqueezeBackward1>), tensor([[-0.0769,  0.0465, -0.0543, -0.5051,  0.1435,  0.2552,  0.0871,  0.3934,\n",
      "         -0.2179,  0.3444,  0.0830, -0.2704,  0.4366, -0.1080, -0.0964,  0.2890,\n",
      "          0.0278, -0.1493,  0.0626, -0.0261,  0.0958,  0.0039,  0.2341, -0.2266,\n",
      "         -0.0662, -0.1495, -0.2587,  0.3134,  0.1401, -0.2271,  0.3361,  0.3025,\n",
      "         -0.2090,  0.0230,  0.1665,  0.1291, -0.1919,  0.0085, -0.1326, -0.0845,\n",
      "         -0.0096,  0.1843, -0.0875,  0.1256, -0.0788, -0.0729,  0.0711, -0.1775,\n",
      "         -0.1883, -0.3262, -0.0262,  0.1413, -0.0816, -0.1715,  0.4900,  0.2116,\n",
      "         -0.0135,  0.0703,  0.0493, -0.0165, -0.2889,  0.1839, -0.2626,  0.0030,\n",
      "          0.0551,  0.2433,  0.2812,  0.0354, -0.0012,  0.1492,  0.0008,  0.2375,\n",
      "          0.2903, -0.0327, -0.0853, -0.1677, -0.0604, -0.2206,  0.1048,  0.1130,\n",
      "          0.0607,  0.0519, -0.0823, -0.1317, -0.1475, -0.1694,  0.2383,  0.0286,\n",
      "          0.2022,  0.0771, -0.2847,  0.2122,  0.1263, -0.0947, -0.0148,  0.4323,\n",
      "          0.0480, -0.1364, -0.1041,  0.1469]], grad_fn=<SqueezeBackward1>))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "lstm = torch.nn.LSTM(input_size=10,hidden_size=100)\n",
    "embed = torch.nn.Embedding(6,10)\n",
    "\n",
    "inputs = embed.weight\n",
    "\n",
    "prev_c = torch.zeros(1,100)\n",
    "prev_h = torch.zeros(1,100)\n",
    "\n",
    "next_h, next_c = lstm(inputs, (prev_h,prev_c))\n",
    "print(next_h.size())\n",
    "print(next_c)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T14:03:34.989721054Z",
     "start_time": "2023-12-11T14:03:32.846746177Z"
    }
   },
   "id": "b80776564d834ec8"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m all_h, all_h_w \u001B[38;5;241m=\u001B[39m [],[]\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m6\u001B[39m):\n\u001B[0;32m----> 8\u001B[0m     next_h, next_c \u001B[38;5;241m=\u001B[39m \u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mprev_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43mprev_c\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \n\u001B[1;32m      9\u001B[0m     prev_h, prev_c \u001B[38;5;241m=\u001B[39m next_h, next_c\n\u001B[1;32m     11\u001B[0m     all_h\u001B[38;5;241m.\u001B[39mappend(next_h[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/miniconda3/envs/jupyter/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/jupyter/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/jupyter/lib/python3.8/site-packages/torch/nn/modules/rnn.py:868\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m    866\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg)\n\u001B[1;32m    867\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 868\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hx[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mhx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdim\u001B[49m() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    869\u001B[0m         msg \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFor unbatched 2-D input, hx and cx should \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    870\u001B[0m                \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124malso be 2-D but got (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhx[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdim()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-D, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhx[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mdim()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-D) tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    871\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "w_attn1 = torch.nn.Linear(in_features=100,out_features=100)\n",
    "w_attn2 = torch.nn.Linear(in_features=100,out_features=100)\n",
    "w_v = torch.nn.Linear(in_features=100,out_features=100)\n",
    "\n",
    "all_h, all_h_w = [],[]\n",
    "\n",
    "for i in range(6):\n",
    "    next_h, next_c = lstm(inputs, (prev_h,prev_c)) \n",
    "    prev_h, prev_c = next_h, next_c\n",
    "    \n",
    "    all_h.append(next_h[-1])\n",
    "    all_h_w.append(w_attn1(next_h[-1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T14:03:35.439697963Z",
     "start_time": "2023-12-11T14:03:34.990287507Z"
    }
   },
   "id": "eb99ad5baa2c0918"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e8bcaf9d5faf034c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
